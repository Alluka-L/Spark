{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Fundamentals with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Big Data analysis with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'3.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version\n",
    "sc.pythonVer\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb = range(1, 101)\n",
    "spark_data = sc.parallelize(numb)\n",
    "spark_data\n",
    "type(spark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "../README.md MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines\n",
    "type(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming in PySpark RDDâ€™s\n",
    "### RDDs from Parallelized collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of RDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD from a list of words\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))\n",
    "RDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs from External Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file_path is ./Big Data Fundamentals with PySpark/data/ham.txt\n",
      "The file type of fileRDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Print the file_path\n",
    "file_path = './Big Data Fundamentals with PySpark/data/ham.txt'\n",
    "print(\"The file_path is\", file_path)\n",
    "\n",
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in fileRDD is 2\n",
      "Number of partitions in fileRDD_part is 5\n"
     ]
    }
   ],
   "source": [
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map and Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n",
      "1000\n",
      "1331\n",
      "1728\n",
      "2197\n",
      "2744\n",
      "3375\n",
      "4096\n",
      "4913\n",
      "5832\n",
      "6859\n",
      "8000\n",
      "9261\n",
      "10648\n",
      "12167\n",
      "13824\n",
      "15625\n",
      "17576\n",
      "19683\n",
      "21952\n",
      "24389\n",
      "27000\n",
      "29791\n",
      "32768\n",
      "35937\n",
      "39304\n",
      "42875\n",
      "46656\n",
      "50653\n",
      "54872\n",
      "59319\n",
      "64000\n",
      "68921\n",
      "74088\n",
      "79507\n",
      "85184\n",
      "91125\n",
      "97336\n",
      "103823\n",
      "110592\n",
      "117649\n",
      "125000\n",
      "132651\n",
      "140608\n",
      "148877\n",
      "157464\n",
      "166375\n",
      "175616\n",
      "185193\n",
      "195112\n",
      "205379\n",
      "216000\n",
      "226981\n",
      "238328\n",
      "250047\n",
      "262144\n",
      "274625\n",
      "287496\n",
      "300763\n",
      "314432\n",
      "328509\n",
      "343000\n",
      "357911\n",
      "373248\n",
      "389017\n",
      "405224\n",
      "421875\n",
      "438976\n",
      "456533\n",
      "474552\n",
      "493039\n",
      "512000\n",
      "531441\n",
      "551368\n",
      "571787\n",
      "592704\n",
      "614125\n",
      "636056\n",
      "658503\n",
      "681472\n",
      "704969\n",
      "729000\n",
      "753571\n",
      "778688\n",
      "804357\n",
      "830584\n",
      "857375\n",
      "884736\n",
      "912673\n",
      "941192\n",
      "970299\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = spark_data.map(lambda x: x ** 3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines with the keyword Spark is 815\n",
      "if you aren't here in the next  &lt;#&gt;  hours imma flip my shit\n",
      "No. I meant the calculation is the same. That  &lt;#&gt; units at  &lt;#&gt; . This school is really expensive. Have you started practicing your accent. Because its important. And have you decided if you are doing 4years of dental school or if you'll just do the nmde exam.\n",
      "I'm taking derek &amp; taylor to walmart, if I'm not back by the time you're done just leave the mouse on my desk and I'll text you when priscilla's ready\n",
      "I can't believe how attached I am to seeing you every day. I know you will do the best you can to get to me babe. I will go to teach my class at your midnight\n"
     ]
    }
   ],
   "source": [
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'the' in line.split(' '))\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4): \n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReduceBykey and Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n"
     ]
    }
   ],
   "source": [
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (4, 5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortByKey and Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 1 has 2 Counts\n"
     ]
    }
   ],
   "source": [
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountingBykeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (3, 6), (4, 5)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of total is <class 'collections.defaultdict'>\n",
      "key 1 has 1 counts\n",
      "key 3 has 2 counts\n",
      "key 4 has 1 counts\n"
     ]
    }
   ],
   "source": [
    "# Transform the rdd with countByKey()\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "    print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a base RDD and transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in splitRDD: 194074\n"
     ]
    }
   ],
   "source": [
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile('./Big Data Fundamentals with PySpark/data/Complete_Shakespeare.txt')\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he',\n",
    "'him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves',\n",
    "'what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have',\n",
    "'has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while',\n",
    "'of','at','by','for','with','about','against','between','into','through','during','before','after','above','below',\n",
    "'to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when',\n",
    "'where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor','not','only',\n",
    "'own','same','so','than','too','very','can','will','just','don','should','now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the words in lower case and remove stop words from stop_words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 9)\n",
      "('EBook', 1)\n",
      "('Shakespeare', 12)\n",
      "('', 65498)\n",
      "('use', 38)\n",
      "('anyone', 1)\n",
      "('anywhere', 1)\n",
      "('restrictions', 1)\n",
      "('whatsoever.', 1)\n",
      "('may', 162)\n",
      " has 65498 counts\n",
      "thou has 650 counts\n",
      "thy has 574 counts\n",
      "shall has 393 counts\n",
      "would has 311 counts\n",
      "good has 295 counts\n",
      "thee has 286 counts\n",
      "love has 273 counts\n",
      "Enter has 269 counts\n",
      "th' has 254 counts\n"
     ]
    }
   ],
   "source": [
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "    print(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
